package com.example.vpntest.hev

import android.util.Log
import kotlinx.coroutines.*
import kotlinx.coroutines.flow.MutableStateFlow
import kotlinx.coroutines.flow.StateFlow
import kotlinx.coroutines.flow.asStateFlow
import java.util.concurrent.atomic.AtomicLong

/**
 * HEV Tunnel ç›£æ§å™¨
 * 
 * è² è²¬ç›£æ§ tunnel ç‹€æ…‹ã€è™•ç†ç•°å¸¸é‡å•Ÿå’Œæ•ˆèƒ½çµ±è¨ˆ
 */
class TunnelMonitor(private val hevTunnelManager: HevTunnelManager) {
    
    companion object {
        private const val TAG = "TunnelMonitor"
        private const val MONITOR_INTERVAL_MS = 5000L
        private const val HEALTH_CHECK_INTERVAL_MS = 10000L
        private const val MAX_RESTART_ATTEMPTS = 3
        private const val RESTART_COOLDOWN_MS = 30000L
    }
    
    private val monitorScope = CoroutineScope(Dispatchers.IO + SupervisorJob())
    private var monitorJob: Job? = null
    private var healthCheckJob: Job? = null
    
    private val _status = MutableStateFlow(TunnelStatus.STOPPED)
    val status: StateFlow<TunnelStatus> = _status.asStateFlow()
    
    // çµ±è¨ˆè³‡è¨Š
    private val statusChangeCount = AtomicLong(0)
    private val crashCount = AtomicLong(0)
    private val restartAttempts = AtomicLong(0)
    private val lastRestartTime = AtomicLong(0)
    private val monitorStartTime = AtomicLong(0)
    
    private var restartCallback: (suspend () -> Boolean)? = null
    
    /**
     * è¨­å®šé‡å•Ÿå›èª¿å‡½æ•¸
     */
    fun setRestartCallback(callback: suspend () -> Boolean) {
        restartCallback = callback
        Log.d(TAG, "ğŸ”„ Restart callback configured")
    }
    
    /**
     * é–‹å§‹ç›£æ§
     */
    fun startMonitoring() {
        if (monitorJob?.isActive == true) {
            Log.w(TAG, "âš ï¸ Monitor already running")
            return
        }
        
        Log.i(TAG, "ğŸ” Starting HEV tunnel monitor")
        monitorStartTime.set(System.currentTimeMillis())
        
        monitorJob = monitorScope.launch {
            updateStatus(TunnelStatus.MONITORING)
            
            while (isActive) {
                try {
                    performStatusCheck()
                    delay(MONITOR_INTERVAL_MS)
                } catch (e: CancellationException) {
                    Log.d(TAG, "ğŸ›‘ Monitor cancelled")
                    break
                } catch (e: Exception) {
                    Log.e(TAG, "âŒ Monitor error", e)
                    delay(1000) // éŒ¯èª¤æ™‚çŸ­æš«å»¶é²
                }
            }
        }
        
        // å•Ÿå‹•å¥åº·æª¢æŸ¥
        startHealthCheck()
    }
    
    /**
     * åœæ­¢ç›£æ§
     */
    fun stopMonitoring() {
        Log.i(TAG, "ğŸ›‘ Stopping HEV tunnel monitor")
        
        monitorJob?.cancel()
        monitorJob = null
        
        healthCheckJob?.cancel()
        healthCheckJob = null
        
        updateStatus(TunnelStatus.STOPPED)
        
        val uptime = System.currentTimeMillis() - monitorStartTime.get()
        Log.i(TAG, "âœ… Monitor stopped (uptime: ${uptime / 1000}s)")
    }
    
    /**
     * åŸ·è¡Œç‹€æ…‹æª¢æŸ¥
     */
    private suspend fun performStatusCheck() {
        val isRunning = hevTunnelManager.isRunning()
        val currentStatus = _status.value
        
        Log.v(TAG, "ğŸ” Status check: running=$isRunning, current=$currentStatus")
        
        when (currentStatus) {
            TunnelStatus.MONITORING -> {
                if (isRunning) {
                    updateStatus(TunnelStatus.RUNNING)
                    Log.i(TAG, "âœ… Tunnel detected as running")
                }
            }
            
            TunnelStatus.RUNNING -> {
                if (!isRunning) {
                    Log.w(TAG, "ğŸ’¥ Tunnel process died unexpectedly")
                    crashCount.incrementAndGet()
                    updateStatus(TunnelStatus.CRASHED)
                    handleTunnelCrash()
                }
            }
            
            TunnelStatus.RESTARTING -> {
                if (isRunning) {
                    updateStatus(TunnelStatus.RUNNING)
                    Log.i(TAG, "âœ… Tunnel restart successful")
                    restartAttempts.set(0) // é‡ç½®é‡å•Ÿè¨ˆæ•¸
                }
            }
            
            else -> {
                // å…¶ä»–ç‹€æ…‹ä¸éœ€è¦ç‰¹æ®Šè™•ç†
            }
        }
    }
    
    /**
     * å•Ÿå‹•å¥åº·æª¢æŸ¥
     */
    private fun startHealthCheck() {
        healthCheckJob = monitorScope.launch {
            while (isActive) {
                try {
                    delay(HEALTH_CHECK_INTERVAL_MS)
                    performHealthCheck()
                } catch (e: CancellationException) {
                    break
                } catch (e: Exception) {
                    Log.e(TAG, "âŒ Health check error", e)
                }
            }
        }
    }
    
    /**
     * åŸ·è¡Œå¥åº·æª¢æŸ¥
     */
    private suspend fun performHealthCheck() {
        if (_status.value == TunnelStatus.RUNNING) {
            try {
                // æª¢æŸ¥éŒ¯èª¤ç¢¼
                val errorCode = hevTunnelManager.getLastErrorCode()
                if (errorCode != HevTunnelManager.ERROR_NONE) {
                    Log.w(TAG, "âš ï¸ Tunnel health check failed: ${HevTunnelManager.getErrorMessage(errorCode)}")
                }
                
                // ç²å–çµ±è¨ˆè³‡è¨Š
                val stats = hevTunnelManager.getStats()
                Log.v(TAG, "ğŸ“Š Health check: $stats")
                
            } catch (e: Exception) {
                Log.e(TAG, "âŒ Health check exception", e)
            }
        }
    }
    
    /**
     * è™•ç† tunnel å´©æ½°
     */
    private suspend fun handleTunnelCrash() {
        val attempts = restartAttempts.incrementAndGet()
        val currentTime = System.currentTimeMillis()
        val timeSinceLastRestart = currentTime - lastRestartTime.get()
        
        Log.w(TAG, "ğŸ’¥ Handling tunnel crash (attempt $attempts/$MAX_RESTART_ATTEMPTS)")
        
        // æª¢æŸ¥é‡å•Ÿå†·å»æ™‚é–“
        if (timeSinceLastRestart < RESTART_COOLDOWN_MS) {
            Log.w(TAG, "â³ Restart cooldown active, waiting...")
            updateStatus(TunnelStatus.WAITING)
            delay(RESTART_COOLDOWN_MS - timeSinceLastRestart)
        }
        
        // æª¢æŸ¥æœ€å¤§é‡å•Ÿæ¬¡æ•¸
        if (attempts > MAX_RESTART_ATTEMPTS) {
            Log.e(TAG, "âŒ Max restart attempts exceeded, giving up")
            updateStatus(TunnelStatus.FAILED)
            return
        }
        
        try {
            updateStatus(TunnelStatus.RESTARTING)
            lastRestartTime.set(currentTime)
            
            Log.i(TAG, "ğŸ”„ Attempting tunnel restart...")
            val success = restartCallback?.invoke() ?: false
            
            if (success) {
                Log.i(TAG, "âœ… Tunnel restart initiated successfully")
                // ç‹€æ…‹æœƒåœ¨ä¸‹ä¸€æ¬¡æª¢æŸ¥æ™‚æ›´æ–°ç‚º RUNNING
            } else {
                Log.e(TAG, "âŒ Tunnel restart failed")
                updateStatus(TunnelStatus.FAILED)
            }
        } catch (e: Exception) {
            Log.e(TAG, "âŒ Exception during tunnel restart", e)
            updateStatus(TunnelStatus.FAILED)
        }
    }
    
    /**
     * æ›´æ–°ç‹€æ…‹
     */
    private fun updateStatus(newStatus: TunnelStatus) {
        val oldStatus = _status.value
        if (oldStatus != newStatus) {
            _status.value = newStatus
            statusChangeCount.incrementAndGet()
            Log.i(TAG, "ğŸ”„ Status changed: $oldStatus â†’ $newStatus")
        }
    }
    
    /**
     * å¼·åˆ¶é‡å•Ÿ
     */
    suspend fun forceRestart(): Boolean {
        Log.i(TAG, "ğŸ”§ Force restart requested")
        restartAttempts.set(0) // é‡ç½®è¨ˆæ•¸å™¨
        updateStatus(TunnelStatus.CRASHED)
        handleTunnelCrash()
        return _status.value == TunnelStatus.RUNNING
    }
    
    /**
     * ç²å–ç›£æ§çµ±è¨ˆ
     */
    fun getMonitorStats(): String {
        val uptime = System.currentTimeMillis() - monitorStartTime.get()
        return buildString {
            appendLine("=== Tunnel Monitor Stats ===")
            appendLine("ğŸ“Š Current Status: ${_status.value}")
            appendLine("â±ï¸ Monitor Uptime: ${uptime / 1000}s")
            appendLine("ğŸ”„ Status Changes: ${statusChangeCount.get()}")
            appendLine("ğŸ’¥ Crash Count: ${crashCount.get()}")
            appendLine("ğŸ”„ Restart Attempts: ${restartAttempts.get()}")
            appendLine("â° Last Restart: ${if (lastRestartTime.get() > 0) "${(System.currentTimeMillis() - lastRestartTime.get()) / 1000}s ago" else "Never"}")
        }
    }
    
    /**
     * æ¸…ç†è³‡æº
     */
    fun cleanup() {
        Log.i(TAG, "ğŸ§¹ Cleaning up tunnel monitor...")
        stopMonitoring()
        monitorScope.cancel()
        Log.i(TAG, "âœ… Tunnel monitor cleanup completed")
    }
}

/**
 * Tunnel ç‹€æ…‹æšèˆ‰
 */
enum class TunnelStatus {
    STOPPED,        // å·²åœæ­¢
    MONITORING,     // ç›£æ§ä¸­
    RUNNING,        // é‹è¡Œä¸­
    CRASHED,        // å·²å´©æ½°
    RESTARTING,     // é‡å•Ÿä¸­
    WAITING,        // ç­‰å¾…ä¸­ï¼ˆå†·å»æœŸï¼‰
    FAILED          // å¤±æ•—
}